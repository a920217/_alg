# Backpropagation  
## & Gradient Engine

---

## 為什麼需要反向傳播？

- 深度神經網路參數數量龐大
- 無法手動計算所有偏導數
- 需要自動化的誤差修正機制

👉 解法：**Backpropagation**

---

## 核心概念：損失函數

- 衡量模型預測與真實值的差距
- 訓練目標：最小化 Loss
- 常見例子：MSE、Cross Entropy

---

## 核心概念：連鎖律

- 反向傳播的數學基礎
- 將輸出誤差逐層回傳
- 每一層只需計算「局部導數」

---

## 梯度下降法

\[
w_{new} = w_{old} - \eta \cdot \frac{\partial L}{\partial w}
\]

- 梯度：函數上升最快方向
- 更新方向：梯度反方向
- \( \eta \)：學習率

---

## 梯度引擎流程

1. Forward Pass
2. 建立運算圖
3. 計算局部梯度
4. Backward Pass
5. 更新權重

---

## 自動微分（Autograd）

- PyTorch Autograd
- TensorFlow GradientTape

特點：
- 動態計算圖
- 自動記憶體回收
- 支援高階導數

---

## 常見問題

### 梯度消失
- 深層網路訓練困難
- 解法：ReLU、ResNet

### 梯度爆炸
- 梯度過大
- 解法：Gradient Clipping

---

## 總結

- Backpropagation 是深度學習核心
- 梯度引擎讓訓練可規模化
- 支撐現代大型模型的基礎技術

**Thank You**
